# Papers

1. **K-BERT: Enabling Language Representation with Knowledge Graph** *AAAI2020 (Liu, Zhou et al. 2019)* [paper](https://arxiv.org/abs/1909.07606), [code](https://github.com/autoliuweijie/K-BERT)
2. **Knowledge enhanced contextual word representations** *EMNLP2019 (Peters, Neumann et al. 2019)* [paper](https://arxiv.org/abs/1909.04164), [code](https://github.com/allenai/kb)
3. **KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation** *arXiv2019 (Wang, Gao et al. 2019)* [paper](https://arxiv.org/abs/1911.06136)
4. **Enriching BERT with Knowledge Graph Embeddings for Document Classification** *arXiv2019 (Ostendorff, Bourgonje et al. 2019)* [paper](https://arxiv.org/abs/1909.08402), [code](https://github.com/malteos/pytorch-bert-document-classification)
5. **ERNIE: Enhanced Language Representation with Informative Entities** *ACL2019 (Zhang, Han et al. 2019)* [paper](https://arxiv.org/abs/1905.07129), [code](https://github.com/thunlp/ERNIE)
6. **ERNIE: Enhanced Representation through Knowledge Integration** *ACL2019 (Sun, Wang et al. 2019)* [paper](https://arxiv.org/abs/1904.09223), [code](https://github.com/PaddlePaddle/ERNIE)
7. **Integrating Graph Contextualized Knowledge into Pre-trained Language Models** *arXiv2019 (He, Zhou et al. 2019)* [paper](https://arxiv.org/abs/1912.00147)
8. **PRETRAINED ENCYCLOPEDIA: WEAKLY SUPERVISED KNOWLEDGE-PRETRAINED LANGUAGE MODEL** *ICLR2020 (Xiong, Du et al. 2019)* [paper](https://arxiv.org/abs/1912.09637)
9. **Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity** *(Lauscher, Vulic et al. 2020)* [paper](https://arxiv.org/abs/1909.02339)
10. **SenseBERT: Driving Some Sense into BERT** *arXiv2020 (Levine, Lenz et al. 2019)* [paper](https://arxiv.org/abs/1908.05646)
11. **K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters** *arXiv2020 (Wang, Tang et al. 2020)* [paper](https://arxiv.org/abs/2002.01808)
12. **Pre-trained Models for Natural Language Processing: A Survey** *arXiv2020 (Qiu, Sun et al. 2020)* *4.1 Knowledge-Enriched PTMs* [paper](https://arxiv.org/abs/2003.08271)
13. **SciBERT: A Pretrained Language Model for Scientific Text** *EMNLP2019 (Beltagy, Lo et al. 2019)* [paper](https://arxiv.org/abs/1903.10676), [code](https://github.com/allenai/scibert/)
14. **BioBERT: a pre-trained biomedical language representation model for biomedical text mining** *arXiv2019 (Lee, Yoon et al. 2020)*  [paper](https://arxiv.org/abs/1901.08746), [code](https://github.com/dmis-lab/biobert), [model](https://github.com/naver/biobert-pretrained)
15. **Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks** *ACL2020 (Gururangan, Marasović et al. 2020)* [paper](https://arxiv.org/abs/2004.10964), [code](https://github.com/allenai/dont-stop-pretraining) 

---
* DeBERTa: Decoding-enhanced BERT with Disentangled Attention

---

# Blog

1. [美团BERT的探索和实践](https://tech.meituan.com/2019/11/14/nlp-bert-practice.html)

