# Papers on bert with knowledge


1. **K-BERT: Enabling Language Representation with Knowledge Graph** *AAAI2020 (Liu, Zhou et al. 2019)* [paper](https://arxiv.org/abs/1909.07606), [code](https://github.com/autoliuweijie/K-BERT)
2. **Knowledge enhanced contextual word representations** *EMNLP2019 (Peters, Neumann et al. 2019)* [paper](https://arxiv.org/abs/1909.04164), [code](https://github.com/allenai/kb)
3. **KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation** *arXiv2019 (Wang, Gao et al. 2019)* [paper](https://arxiv.org/abs/1911.06136)
4. **Enriching BERT with Knowledge Graph Embeddings for Document Classification** *arXiv2019 (Ostendorff, Bourgonje et al. 2019)* [paper](https://arxiv.org/abs/1909.08402), [code](https://github.com/malteos/pytorch-bert-document-classification)
5. **ERNIE: Enhanced Language Representation with Informative Entities** *ACL2019 (Zhang, Han et al. 2019)* [paper](https://arxiv.org/abs/1905.07129), [code](https://github.com/thunlp/ERNIE)
6. **ERNIE: Enhanced Representation through Knowledge Integration** *ACL2019 (Sun, Wang et al. 2019)* [paper](https://arxiv.org/abs/1904.09223), [code](https://github.com/PaddlePaddle/ERNIE)
7. **Integrating Graph Contextualized Knowledge into Pre-trained Language Models** *arXiv2019 (He, Zhou et al. 2019)* [paper](https://arxiv.org/abs/1912.00147)
8. **PRETRAINED ENCYCLOPEDIA: WEAKLY SUPERVISED KNOWLEDGE-PRETRAINED LANGUAGE MODEL** *ICLR2020 (Xiong, Du et al. 2019)* [paper](https://arxiv.org/abs/1912.09637)
9. **Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity** *(Lauscher, Vulic et al. 2020)* [paper](https://arxiv.org/abs/1909.02339)
10. **SenseBERT: Driving Some Sense into BERT** *arXiv2020 (Levine, Lenz et al. 2019)* [paper](https://arxiv.org/abs/1908.05646)
11. **K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters** *arXiv2020 (Wang, Tang et al. 2020)* [paper](https://arxiv.org/abs/2002.01808)
12. **Pre-trained Models for Natural Language Processing: A Survey** *arXiv2020 (Qiu, Sun et al. 2020)* *4.1 Knowledge-Enriched PTMs* [paper](https://arxiv.org/abs/2003.08271)
